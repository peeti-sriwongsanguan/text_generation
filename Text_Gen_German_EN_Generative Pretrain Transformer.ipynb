{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae566112-3b05-4982-848e-36477c5ee081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 10:03:00.890898: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#@title Imports\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "from transformers import T5Tokenizer, TFT5Model, TFT5ForConditionalGeneration\n",
    "from transformers import GPT2Tokenizer, TFOPTForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d89d8a9-cae1-4903-b34a-95bc4ae68e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/peeti_mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d7f21-ccab-4b41-9b6e-47cfc46570fa",
   "metadata": {},
   "source": [
    "## Building a Seq2Seq model for Translation using RNNs with and without Attention\n",
    "\n",
    "### Downloading and pre-processing Data\n",
    "\n",
    "\n",
    "Let's get the data. Just like the Keras tutorial, we will use http://www.manythings.org as the source for the parallel corpus, but we will use German.  Machine translation requires sentence pairs for training, that is individual sentences in German and the corresponding sentence in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d74b63ee-946b-416d-9923-228021fdbe9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Archive:  deu-eng.zip',\n",
       " 'replace deu.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL',\n",
       " '(EOF or read error, treating as \"[N]one\" ...)']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!curl -O http://www.manythings.org/anki/deu-eng.zip\n",
    "!!unzip deu-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24d1aa-0f7a-426c-a54e-b1f2619585a6",
   "metadata": {},
   "source": [
    "Note these numbers are much smaller than the real world plus I am working on cpu machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22119dda-3128-41f3-aedb-c0634ebf64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 100  # Embedding dimensions for vectors and LSTMs.\n",
    "num_samples = 10000  # Number of examples to consider.\n",
    "\n",
    "# Path to the data txt file on disk.\n",
    "data_path = \"deu.txt\"\n",
    "\n",
    "# Vocabulary sizes that we'll use:\n",
    "english_vocab_size = 2000\n",
    "german_vocab_size = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bccc87-fc70-4647-b708-f0b4b5795c26",
   "metadata": {},
   "source": [
    "Next we need to format the input by using nltk for the tokenization.\n",
    "\n",
    "using CountVectorizer to create a vocabulary from the most frequent words in each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1a8a66-cd79-4a2d-9bb4-e24a1b0ba601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum source input length:  6\n",
      "Maximum target output length:  10\n"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "max_input_length = -1\n",
    "max_output_length = -1\n",
    "\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "\n",
    "    tokenized_source_text = nltk.word_tokenize(input_text, language='english')\n",
    "    tokenized_target_text = nltk.word_tokenize(target_text, language='german')\n",
    "\n",
    "    if len(tokenized_source_text) > max_input_length:\n",
    "      max_input_length = len(tokenized_source_text)\n",
    "\n",
    "    if len(tokenized_target_text) > max_output_length:\n",
    "      max_output_length = len(tokenized_target_text)\n",
    "\n",
    "\n",
    "    source_text = (' '.join(tokenized_source_text)).lower()\n",
    "    target_text = (' '.join(tokenized_target_text)).lower()\n",
    "\n",
    "    input_texts.append(source_text)\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "vectorizer_english = CountVectorizer(max_features=english_vocab_size)\n",
    "vectorizer_english.fit(input_texts)\n",
    "vocab_english = vectorizer_english.get_feature_names_out()\n",
    "\n",
    "vectorizer_german = CountVectorizer(max_features=german_vocab_size)\n",
    "vectorizer_german.fit(target_texts)\n",
    "vocab_german = vectorizer_german.get_feature_names_out()\n",
    "\n",
    "print('Maximum source input length: ', max_input_length)\n",
    "print('Maximum target output length: ', max_output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63545a20-9a08-418d-9e86-ea65733a1c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go .', 'hi .']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at a few input words\n",
    "\n",
    "input_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "228b73dc-5c04-4017-be7f-b39aeafa49fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geh .', 'hallo !']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here with german translation \n",
    "\n",
    "target_texts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee15cbb-cb8b-4ca6-bc59-a2c9616550d8",
   "metadata": {},
   "source": [
    "from our source and target sequences above, we set our max lengths 6 and 11, respectively. As we will add start and end tokens (\\<s> and \\</s>) to our decoder side we will set the respective max lengths to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8779144b-a3ae-41cf-8467-5fb6aec56561",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = 6\n",
    "max_decoder_seq_length = 13 #11 + start + end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49cc5d-0d47-4e8a-a889-3d2b1b25b5b6",
   "metadata": {},
   "source": [
    "Next, we create the dictionaries translating between integer ids and tokens for both source (English) and target (German)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "381b4f04-da56-48fe-a7bd-b7ac2eb11d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id_vocab_dict = {}\n",
    "source_vocab_id_dict = {}\n",
    "\n",
    "for sid, svocab in enumerate(vocab_english):\n",
    "  source_id_vocab_dict[sid] = svocab\n",
    "  source_vocab_id_dict[svocab] = sid\n",
    "\n",
    "source_id_vocab_dict[english_vocab_size] = \"<unk>\"\n",
    "source_id_vocab_dict[english_vocab_size + 1] = \"<pad>\"\n",
    "\n",
    "source_vocab_id_dict[\"<unk>\"] = english_vocab_size\n",
    "source_vocab_id_dict[\"<pad>\"] = english_vocab_size + 1\n",
    "\n",
    "target_id_vocab_dict = {}\n",
    "target_vocab_id_dict = {}\n",
    "\n",
    "for tid, tvocab in enumerate(vocab_german):\n",
    "  target_id_vocab_dict[tid] = tvocab\n",
    "  target_vocab_id_dict[tvocab] = tid\n",
    "\n",
    "# Add unknown token plus start and end tokens to target language\n",
    "\n",
    "target_id_vocab_dict[german_vocab_size] = \"<unk>\"\n",
    "target_id_vocab_dict[german_vocab_size + 1] = \"<start>\"\n",
    "target_id_vocab_dict[german_vocab_size + 2] = \"<end>\"\n",
    "target_id_vocab_dict[german_vocab_size + 3] = \"<pad>\"\n",
    "\n",
    "target_vocab_id_dict[\"<unk>\"] = german_vocab_size\n",
    "target_vocab_id_dict[\"<start>\"] = german_vocab_size + 1\n",
    "target_vocab_id_dict[\"<end>\"] = german_vocab_size + 2\n",
    "target_vocab_id_dict[\"<pad>\"] = german_vocab_size + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538967a8-ec78-4fd9-b2fc-853a70d78223",
   "metadata": {},
   "source": [
    "Lastly, we need to create the training and test data that will feed into our two models. It is convenient to define a small function for that that also takes care off padding and adding start/end tokens on the decoder side.\n",
    "\n",
    "Notice that we need to create three sequences of vocab ids: inputs to the encoder (starting language), inputs to the decoder (output language, for the preceding tokens in the output sequence) and labels for the decoder (the correct next word to predict at each time step in the output, which is shifted one over from the inputs to the decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc136ab6-d3b6-4a4e-90a8-7e67f4c653c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_data(texts, \n",
    "                         vocab_id_dict, \n",
    "                         max_length=20, \n",
    "                         type=None,\n",
    "                         train_test_vector=None,\n",
    "                         samples=100000):\n",
    "  \n",
    "  if type == None:\n",
    "    raise ValueError('\\'type\\' is not defined. Please choose from: input_source, input_target, output_target.')\n",
    "  \n",
    "  train_data = []\n",
    "  test_data = []\n",
    "\n",
    "  for text_num, text in enumerate(texts[:samples]):\n",
    "\n",
    "    sentence_ids = []\n",
    "\n",
    "    for token in text.split():\n",
    "\n",
    "      if token in vocab_id_dict.keys():\n",
    "        sentence_ids.append(vocab_id_dict[token])\n",
    "      else:\n",
    "        sentence_ids.append(vocab_id_dict[\"<unk>\"])\n",
    "    \n",
    "    vocab_size = len(vocab_id_dict.keys())\n",
    "    \n",
    "    # Depending on encoder/decoder and input/output, add start/end tokens.\n",
    "    # Then add padding.\n",
    "    \n",
    "    if type == 'input_source':\n",
    "      ids = (sentence_ids + [vocab_size - 1] * max_length)[:max_length]\n",
    "\n",
    "    elif type == 'input_target':\n",
    "      ids = ([vocab_size -3] + sentence_ids + [vocab_size - 2] + [vocab_size - 1] * max_length)[:max_length]\n",
    "\n",
    "    elif type == 'output_target':\n",
    "      ids = (sentence_ids + [vocab_size - 2] + [vocab_size -1] * max_length)[:max_length]\n",
    "\n",
    "    if train_test_vector is not None and not train_test_vector[text_num]:\n",
    "      test_data.append(ids)\n",
    "    else:\n",
    "      train_data.append(ids)\n",
    "\n",
    "\n",
    "  return np.array(train_data), np.array(test_data)\n",
    "\n",
    "\n",
    "train_test_split_vector = (np.random.uniform(size=10000) > 0.2)\n",
    "\n",
    "train_source_input_data, test_source_input_data = convert_text_to_data(input_texts, \n",
    "                                                                       source_vocab_id_dict,\n",
    "                                                                       type='input_source',\n",
    "                                                                       max_length=max_encoder_seq_length,\n",
    "                                                                       train_test_vector=train_test_split_vector)\n",
    "\n",
    "train_target_input_data, test_target_input_data = convert_text_to_data(target_texts,\n",
    "                                                                       target_vocab_id_dict,\n",
    "                                                                       type='input_target',\n",
    "                                                                       max_length=max_decoder_seq_length,\n",
    "                                                                       train_test_vector=train_test_split_vector)\n",
    "\n",
    "train_target_output_data, test_target_output_data = convert_text_to_data(target_texts,\n",
    "                                                                         target_vocab_id_dict,\n",
    "                                                                         type='output_target',\n",
    "                                                                         max_length=max_decoder_seq_length,\n",
    "                                                                         train_test_vector=train_test_split_vector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c77d16-f492-468e-9064-55b3c759c32d",
   "metadata": {},
   "source": [
    "**T5 system** was introduced for generating text with transformer. This model uses both the encoder and the decoder configurations of transformers and connects them together. A big difference with this model is that it designed to accept text as an input and produce text as an output for a number of different tasks ranging from summarization and question answering to classification. The system needs to be told which task to perform as the first part of the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c5179c4-a8a8-490a-a213-0453b43c6d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9f2c1917664b0889e609563d0d3fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfa5db53d1047ba947cb85f74462acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peeti_mac/opt/anaconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:220: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tft5_for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (Embedding)          multiple                  32899072  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  334939648 \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  435627520 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 737668096 (2.75 GB)\n",
      "Trainable params: 737668096 (2.75 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "\n",
    "t5_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c99df-1e4b-479e-ab27-5cf80ffc28a0",
   "metadata": {},
   "source": [
    "lets start with some examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70877f73-9e60-43a1-8ee7-394f20a4e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = (\"Oh boy, what a lengthy and cumbersome excercise this was. \" \\\n",
    "           \"I had to look into every detail, check everything twice, \" \\\n",
    "           \" and then compare to prior results. Because of this tediousness \" \\\n",
    "           \" and extra work my homework was 2 days late.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75249cf6-b50b-4543-acfe-39da2aeab6a8",
   "metadata": {},
   "source": [
    "we need to specify the task we want T5 to perform and include it at the begining of the input text. We add a task prompt to the begining of our input. Because we are summarizing, we add the word summarize: to the begining of our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c0ccd28-258d-40e0-bcf6-1749932005aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_input_text = \"summarize: \" + ARTICLE\n",
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ee9e3-f50f-4f7f-b529-d967f2f05fc8",
   "metadata": {},
   "source": [
    "Not great. But let's get more sophisticated and prescribe a minimum length and use beam search to generate multiple outputs.  We also indicate the maximum length the output should be.  Finally, in order to reduce repetitive output we tell the model to avoid output that repeats trigrams (three word groupings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c02e13b4-d392-4e00-82af-501ab87eb909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peeti_mac/opt/anaconda3/lib/python3.9/site-packages/transformers/generation/tf_utils.py:834: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "2023-10-06 10:04:29.634386: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff046945ce0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-06 10:04:29.634838: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-10-06 10:04:29.638412: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-06 10:04:29.736095: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-06 10:04:30.201211: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['homework was a lengthy and cumbersome excercise . because of this tedious']\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'])\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False)\n",
    "       for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf69c5-41ea-4d63-afee-224994a7af4e",
   "metadata": {},
   "source": [
    "Not great. But let's get more sophisticated and prescribe a minimum length and use beam search to generate multiple outputs.  We also indicate the maximum length the output should be.  Finally, in order to reduce repetitive output we tell the model to avoid output that repeats trigrams (three word groupings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "670c40dd-ebf9-4b38-a185-afef5c798784",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_input_text = \"translate English to German: \" + ARTICLE\n",
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d48e9d-1192-46f1-ab3b-5cb3f47e79fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ich habe es nicht geschafft, meinen ersten Test zu schreiben, da ich nicht genügend Zeit hatte, um meinen Test zu bearbeiten.']\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                   num_beams=3,\n",
    "                                   no_repeat_ngram_size=3,\n",
    "                                   min_length=10,\n",
    "                                   max_length=40)\n",
    "                             \n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e0a70db-2c6a-4259-8285-aa8d8626e394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Das war heute wirklich nicht sehr gut; es war zu schwierig zu lösen.']\n"
     ]
    }
   ],
   "source": [
    "t5_input_text = \"translate English to German: That was really not very good today; it was too difficult to solve.\"\n",
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')\n",
    "\n",
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                   num_beams=3,\n",
    "                                   no_repeat_ngram_size=3,\n",
    "                                   min_length=10,\n",
    "                                   max_length=40)\n",
    "                             \n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d7926-8869-4a71-9d79-fb022bbc7bcb",
   "metadata": {},
   "source": [
    "Hmm... output language fluency is very good. But take the German output and feed it in to translate.google.com and see what this means. Is it anything like its English input? This hallucination might be mitigated by changing some of the hyperparameters like num_beams.\n",
    "\n",
    "Is a shorter example more accurate?  Maybe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
