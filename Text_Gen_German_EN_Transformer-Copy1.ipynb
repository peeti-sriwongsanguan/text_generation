{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae566112-3b05-4982-848e-36477c5ee081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 15:16:48.987580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#@title Imports\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "from transformers import T5Tokenizer, TFT5Model, TFT5ForConditionalGeneration\n",
    "from transformers import GPT2Tokenizer, TFOPTForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d89d8a9-cae1-4903-b34a-95bc4ae68e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/peeti_mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d7f21-ccab-4b41-9b6e-47cfc46570fa",
   "metadata": {},
   "source": [
    "## Building a Seq2Seq model for Translation using RNNs with and without Attention\n",
    "\n",
    "### Downloading and pre-processing Data\n",
    "\n",
    "\n",
    "Let's get the data. Just like the Keras tutorial, we will use http://www.manythings.org as the source for the parallel corpus, but we will use German.  Machine translation requires sentence pairs for training, that is individual sentences in German and the corresponding sentence in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d74b63ee-946b-416d-9923-228021fdbe9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Archive:  deu-eng.zip',\n",
       " 'replace deu.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL',\n",
       " '(EOF or read error, treating as \"[N]one\" ...)']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!curl -O http://www.manythings.org/anki/deu-eng.zip\n",
    "!!unzip deu-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24d1aa-0f7a-426c-a54e-b1f2619585a6",
   "metadata": {},
   "source": [
    "Note these numbers are much smaller than the real world plus I am working on cpu machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22119dda-3128-41f3-aedb-c0634ebf64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 100  # Embedding dimensions for vectors and LSTMs.\n",
    "num_samples = 10000  # Number of examples to consider.\n",
    "\n",
    "# Path to the data txt file on disk.\n",
    "data_path = \"deu.txt\"\n",
    "\n",
    "# Vocabulary sizes that we'll use:\n",
    "english_vocab_size = 2000\n",
    "german_vocab_size = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bccc87-fc70-4647-b708-f0b4b5795c26",
   "metadata": {},
   "source": [
    "Next we need to format the input by using nltk for the tokenization.\n",
    "\n",
    "using CountVectorizer to create a vocabulary from the most frequent words in each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1a8a66-cd79-4a2d-9bb4-e24a1b0ba601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum source input length:  6\n",
      "Maximum target output length:  10\n"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "max_input_length = -1\n",
    "max_output_length = -1\n",
    "\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "\n",
    "    tokenized_source_text = nltk.word_tokenize(input_text, language='english')\n",
    "    tokenized_target_text = nltk.word_tokenize(target_text, language='german')\n",
    "\n",
    "    if len(tokenized_source_text) > max_input_length:\n",
    "      max_input_length = len(tokenized_source_text)\n",
    "\n",
    "    if len(tokenized_target_text) > max_output_length:\n",
    "      max_output_length = len(tokenized_target_text)\n",
    "\n",
    "\n",
    "    source_text = (' '.join(tokenized_source_text)).lower()\n",
    "    target_text = (' '.join(tokenized_target_text)).lower()\n",
    "\n",
    "    input_texts.append(source_text)\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "vectorizer_english = CountVectorizer(max_features=english_vocab_size)\n",
    "vectorizer_english.fit(input_texts)\n",
    "vocab_english = vectorizer_english.get_feature_names_out()\n",
    "\n",
    "vectorizer_german = CountVectorizer(max_features=german_vocab_size)\n",
    "vectorizer_german.fit(target_texts)\n",
    "vocab_german = vectorizer_german.get_feature_names_out()\n",
    "\n",
    "print('Maximum source input length: ', max_input_length)\n",
    "print('Maximum target output length: ', max_output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63545a20-9a08-418d-9e86-ea65733a1c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go .', 'hi .']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look at a few input words\n",
    "\n",
    "input_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "228b73dc-5c04-4017-be7f-b39aeafa49fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geh .', 'hallo !']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here with german translation \n",
    "\n",
    "target_texts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee15cbb-cb8b-4ca6-bc59-a2c9616550d8",
   "metadata": {},
   "source": [
    "from our source and target sequences above, we set our max lengths 6 and 11, respectively. As we will add start and end tokens (\\<s> and \\</s>) to our decoder side we will set the respective max lengths to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8779144b-a3ae-41cf-8467-5fb6aec56561",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = 6\n",
    "max_decoder_seq_length = 13 #11 + start + end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49cc5d-0d47-4e8a-a889-3d2b1b25b5b6",
   "metadata": {},
   "source": [
    "Next, we create the dictionaries translating between integer ids and tokens for both source (English) and target (German)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "381b4f04-da56-48fe-a7bd-b7ac2eb11d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id_vocab_dict = {}\n",
    "source_vocab_id_dict = {}\n",
    "\n",
    "for sid, svocab in enumerate(vocab_english):\n",
    "  source_id_vocab_dict[sid] = svocab\n",
    "  source_vocab_id_dict[svocab] = sid\n",
    "\n",
    "source_id_vocab_dict[english_vocab_size] = \"<unk>\"\n",
    "source_id_vocab_dict[english_vocab_size + 1] = \"<pad>\"\n",
    "\n",
    "source_vocab_id_dict[\"<unk>\"] = english_vocab_size\n",
    "source_vocab_id_dict[\"<pad>\"] = english_vocab_size + 1\n",
    "\n",
    "target_id_vocab_dict = {}\n",
    "target_vocab_id_dict = {}\n",
    "\n",
    "for tid, tvocab in enumerate(vocab_german):\n",
    "  target_id_vocab_dict[tid] = tvocab\n",
    "  target_vocab_id_dict[tvocab] = tid\n",
    "\n",
    "# Add unknown token plus start and end tokens to target language\n",
    "\n",
    "target_id_vocab_dict[german_vocab_size] = \"<unk>\"\n",
    "target_id_vocab_dict[german_vocab_size + 1] = \"<start>\"\n",
    "target_id_vocab_dict[german_vocab_size + 2] = \"<end>\"\n",
    "target_id_vocab_dict[german_vocab_size + 3] = \"<pad>\"\n",
    "\n",
    "target_vocab_id_dict[\"<unk>\"] = german_vocab_size\n",
    "target_vocab_id_dict[\"<start>\"] = german_vocab_size + 1\n",
    "target_vocab_id_dict[\"<end>\"] = german_vocab_size + 2\n",
    "target_vocab_id_dict[\"<pad>\"] = german_vocab_size + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538967a8-ec78-4fd9-b2fc-853a70d78223",
   "metadata": {},
   "source": [
    "Lastly, we need to create the training and test data that will feed into our two models. It is convenient to define a small function for that that also takes care off padding and adding start/end tokens on the decoder side.\n",
    "\n",
    "Notice that we need to create three sequences of vocab ids: inputs to the encoder (starting language), inputs to the decoder (output language, for the preceding tokens in the output sequence) and labels for the decoder (the correct next word to predict at each time step in the output, which is shifted one over from the inputs to the decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc136ab6-d3b6-4a4e-90a8-7e67f4c653c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_data(texts, \n",
    "                         vocab_id_dict, \n",
    "                         max_length=20, \n",
    "                         type=None,\n",
    "                         train_test_vector=None,\n",
    "                         samples=100000):\n",
    "  \n",
    "  if type == None:\n",
    "    raise ValueError('\\'type\\' is not defined. Please choose from: input_source, input_target, output_target.')\n",
    "  \n",
    "  train_data = []\n",
    "  test_data = []\n",
    "\n",
    "  for text_num, text in enumerate(texts[:samples]):\n",
    "\n",
    "    sentence_ids = []\n",
    "\n",
    "    for token in text.split():\n",
    "\n",
    "      if token in vocab_id_dict.keys():\n",
    "        sentence_ids.append(vocab_id_dict[token])\n",
    "      else:\n",
    "        sentence_ids.append(vocab_id_dict[\"<unk>\"])\n",
    "    \n",
    "    vocab_size = len(vocab_id_dict.keys())\n",
    "    \n",
    "    # Depending on encoder/decoder and input/output, add start/end tokens.\n",
    "    # Then add padding.\n",
    "    \n",
    "    if type == 'input_source':\n",
    "      ids = (sentence_ids + [vocab_size - 1] * max_length)[:max_length]\n",
    "\n",
    "    elif type == 'input_target':\n",
    "      ids = ([vocab_size -3] + sentence_ids + [vocab_size - 2] + [vocab_size - 1] * max_length)[:max_length]\n",
    "\n",
    "    elif type == 'output_target':\n",
    "      ids = (sentence_ids + [vocab_size - 2] + [vocab_size -1] * max_length)[:max_length]\n",
    "\n",
    "    if train_test_vector is not None and not train_test_vector[text_num]:\n",
    "      test_data.append(ids)\n",
    "    else:\n",
    "      train_data.append(ids)\n",
    "\n",
    "\n",
    "  return np.array(train_data), np.array(test_data)\n",
    "\n",
    "\n",
    "train_test_split_vector = (np.random.uniform(size=10000) > 0.2)\n",
    "\n",
    "train_source_input_data, test_source_input_data = convert_text_to_data(input_texts, \n",
    "                                                                       source_vocab_id_dict,\n",
    "                                                                       type='input_source',\n",
    "                                                                       max_length=max_encoder_seq_length,\n",
    "                                                                       train_test_vector=train_test_split_vector)\n",
    "\n",
    "train_target_input_data, test_target_input_data = convert_text_to_data(target_texts,\n",
    "                                                                       target_vocab_id_dict,\n",
    "                                                                       type='input_target',\n",
    "                                                                       max_length=max_decoder_seq_length,\n",
    "                                                                       train_test_vector=train_test_split_vector)\n",
    "\n",
    "train_target_output_data, test_target_output_data = convert_text_to_data(target_texts,\n",
    "                                                                         target_vocab_id_dict,\n",
    "                                                                         type='output_target',\n",
    "                                                                         max_length=max_decoder_seq_length,\n",
    "                                                                         train_test_vector=train_test_split_vector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f39755-ef48-4cc7-9ddc-f2477c3a5ef9",
   "metadata": {},
   "source": [
    "# Prompt Engineering and Generative Large Language Models¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a89d6-ccbc-44ed-9c24-5351206cbeeb",
   "metadata": {},
   "source": [
    "The development of very large language models such as [GPT3](https://arxiv.org/pdf/2005.14165.pdf) have led to increased interest in few shot and zero shot approaches to tasks.  These generative language models allow a user to provide a prompt with several examples followed by a question the model must answer.  GPT3, especially its 175 billion parameter model, demonstrates the feasibility of a zero shot model where the model can simply be presented with the prompt and in many instances provide the correct answer.  \n",
    "\n",
    "The implication of this zero shot capability is that a very large generative language model can be pre-trained and then shared by a large group of people because it requires no fine-tuning or parameter manipulation. Instead, the users work on the wording of their prompt and providing enough context that the model an perform the task correctly. [Liu et. al.](https://arxiv.org/pdf/2107.13586.pdf) characterize this as \"pre-train, prompt, and predict.\"\n",
    "\n",
    "There are multiple approaches to pre-train, prompt and predict.  Here we explore two of them.  First we look at cloze prompts.  These leverage the masked language model approach used in BERT an T5 where individual words or spans are masked and during pre-training the model learns to predict the maked tokens. Second we look at prefix prompts.  These leverage the next word prediction capability of decoder only models in the GPT family. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263062d4-ae27-42f8-8b6a-d5b4ee54aa2f",
   "metadata": {},
   "source": [
    "### Cloze Prompts\n",
    "\n",
    "Cloze prompts take advantage of the masked language model task where an individual word or span of words anywhere in the input are masked and the language model learns to predict them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26cd65-d067-4517-9358-f4ac3fc40422",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2ece1-b1ac-44b0-a0e1-384950793fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_SENTENCE = ( \"An Australian <extra_id_0> is a type of working dog .\")\n",
    "t5_input_text = PROMPT_SENTENCE\n",
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')\n",
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'], \n",
    "                                   num_beams=10,\n",
    "                                   #temperature=0.8,\n",
    "                                   no_repeat_ngram_size=2,\n",
    "                                   num_return_sequences=3,\n",
    "                                   min_length=1,\n",
    "                                   max_length=3)\n",
    "                             \n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True, \n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e4fbd0-cc1b-43e9-a07b-6ee4d5598134",
   "metadata": {},
   "source": [
    "\n",
    "\"<extra_id_0>\" is the special token (called a sentinel token) we can use with T5 to invoke its masked word modeling ability. There are up to 99 of these tokens. This means we can construct sentences, like a fill in the blank test, that allow us to probe the knowledge embedded in the model based on its pre-training. Here's an example that works well. After you've run it try substituting beagle for poodle and you'll see the model gets confused.\n",
    "\n",
    "Notice two that we are using a beam search approach and accepting the top three choices rather than just the first choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
